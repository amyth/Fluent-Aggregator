<source>
	## tail config option
	@type solrtail
	read_from_head true
	path /var/log/kombine/kombine.log
	exclude_path ["/var/log/kombine/*.gz", "/var/log/kombine/*.zip"]
	format json
	tag smtp
	time_format %b %d %H:%M:%S
	time_key sent_at
	keep_time_key true
	rotate_wait 15
    pos_file /data/source_logs/kombine.log.pos
	solr_address http://172.22.65.28:8983/solr/cda
	identifier email
	identifier_key sEm
	chunk_size 200
	required_fields id,sEm,sCC,sCIT,sCJEx,sCJExm,sCTT,sCFA,sASN,sEx,sCL,sFLN,sCP,sS,sSExp,sHQL,sHEIN,sHES,sGM,sRD,sLM,sQS,sEV,sCPV,sEAS,sPS,sMAS,sPSS,sPBS,sRPS,sPLID,sLH
    replace_with_nil sLM,sRD

    ## Mongo Configuration
    mongo_merge_enabled true
    mongo_address 172.22.66.236
    mongo_port 27018
    mongo_db sumoplus
    mongo_user sumoplus
    mongo_password $um0pL7su$3>
    mongo_collection_type CandidateStatic
    mongo_query_key e
    mongo_projection_keys svi,evi
    mongo_match_by _id::id

</source>

## Match the smtp tag specified by tail to
## kombine logs and send'em to ES
<match smtp>
	type elasticsearch
	hosts localhost
	port 9200
	logstash_format true
	logstash_prefix logstash-smtpdata
	logstash_dateformat %Y.%m
	type_name postfix
	flush_interval 10s
</match>

## Reads the tail of the solrtail log file
## and tags it with the given tag
<source>
	type tail
	tag candidates
	read_from_head true
	path /var/log/solrtail/solrtail.log
    pos_file /var/log/solrtail/solrtail.log.pos
	format json
	time_key sRD
	keep_time_key true
</source>

## use a filter to change key names for
## elastic search index.
#<filter candidates>
#	@type record_transformer
#	<record>
#		email ${sEm}
#		crnt_cmpny ${sCC}
#		industry ${sCIT}
#		exp_yrs ${sCJEx}
#		exp_mon ${sCJExm}
#		job_title ${sCTT}
#		func_area ${sCFA}
#		ann_slry ${sASN}
#		tot_exp ${sEx}
#		crnt_locat ${sCL}
#		name ${sFLN}
#		phone ${sCP}
#		skill ${sS}
#		skill_exp ${sSExp}
#		hghst_qual ${sHQL}
#		inst ${sHEIN}
#		educ_stream ${sHES}
#		gender ${sGM}
#		reg_comp_dt ${sRD}
#		last_modif ${sLM}
#		last_login ${sLH}
#		qulty_score ${sQS}
#		email_vrfd ${sEV}
#		phone_vrfd ${sCPV}
#		email_alrt ${sEAS}
#		private_profile ${sPS}
#		mobile_alrt ${sMAS}
#		spam ${sPSS}
#		bounce ${sPBS}
#		rcrtr_mail_stng ${sRPS}
#		prfrd_locat ${sPLID}
#       start_vendor ${svi}
#        end_vendor ${evi}
#        u_id ${id}
#	</record>
#	remove_keys sEm,sCC,sCIT,sCJEx,sCJExm,sCTT,sCFA,sASN,sEx,sCL,sFLN,sCP,sS,sSExp,sHQL,sHEIN,sHES,sGM,sRD,sLM,sQS,sEV,sCPV,sEAS,sPS,sMAS,sPSS,sPBS,sRPS,sPLID,sLH,svi,evi,id
#</filter>

## Matches the tag candidates and sends
## the data to elasticsearch
<match candidates>
	type elasticsearch
	hosts localhost
	port 9200
	index_name logdata-solrdata
	type_name candidate
	id_key u_id
	flush_interval 10s
</match>

## Reads the tail from the given log file and
## gets data from given solr address using the
## given identifier and identifier key and logs
## the data into the given solrtail log file
#<source>
#	@type tail
#	tag trackmail
#   read_from_head true
#	path /data/source_logs/trackOpen.log
#    pos_file /data/source_logs/trackOpen.log.pos
#	time_format %d/%b/%Y:%H:%M:%S %z
#    format grok_pure
#    grok_pattern_path /tmp/grok_patterns
#    grok_pattern %{IPORHOST:clientip} [-] \[%{HTTPDATE:timestamp}\]  "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"
#    time_key timestamp
#    keep_time_key true
#</source>

#<filter trackmail>
#    @type record_transformer
#    enable_ruby
#    <record>
#        campdt ${( request.match(/[&?]utm_campdt[=](?<utm_campdt>.*?)(&|$)/) or Hash.new('') )[:utm_campdt]}
#        email ${( request.match(/[&?]user_email[=](?<user_email>.*?)(&|$)/) or Hash.new('') )[:user_email].gsub('%40', '@')}
#        medium ${( request.match(/[&?]utm_medium[=](?<utm_medium>.*?)(&|$)/) or Hash.new('') )[:utm_medium]}
#        source ${( request.match(/[&?]utm_source[=](?<utm_source>.*?)(&|$)/) or Hash.new('') )[:utm_source]}
#        campaign ${( request.match(/[&?]utm_camp[=](?<utm_camp>.*?)(&|$)/) or Hash.new('') )[:utm_camp]}
#        u_id ${( request.match(/[&?]user_id[=](?<user_id>.*?)(&|$)/) or Hash.new('') )[:user_id]}
#    </record>
#    remove_keys rawrequest,request,httpversion
#</filter>

## Matches the tag trackmail and sends
## the data to elasticsearch
#<match trackmail>
#	type elasticsearch
#	hosts localhost
#	port 9200
#	logstash_format true
#	logstash_prefix logstash-mailopens
#	logstash_dateformat %Y.%m
#	type_name mailopen
#	flush_interval 10s
#	time_key timestamp
#</match>

## Reads apache access logs
#<source>
#    @type tail
#    read_from_head true
#    path /data/source_logs/httpd-access.log
#    pos_file /data/source_logs/httpd-access.log.pos
#    tag accesslog
#    format grok_pure
#    grok_pattern_path /Users/amyth/installs/efk/grokpatterns
#    grok_pattern %{SYSLOGBASE:syslogbase}(.*?)%{COMBINEDAPACHELOG:comblog}
#    time_key timestamp
#    time_format %d/%b/%Y:%H:%M:%S %z
#</source>
#
#<match accesslog>
#    @type rewrite_tag_filter
#
#    ## Rewrite clicks
#    rewriterule1 request tc=.* clicks
#
#    ## Rewrite job applications
#    rewriterule2 request ^/myshine/jobs/apply/\d+/$ apply.web
#    rewriterule3 request ^/api.*/job-apply/.*$ apply.api
#</match>
#
### Filter clicks
#<filter clicks>
#    @type record_transformer
#    enable_ruby
#    <record>
#        campdt ${( ( ( request.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[2] or '' ).match(/(?<datepart>.*?)T(.*)/) or Hash.new('') )[:datepart].gsub(/-/, '')}
#        email ${( ( request.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[3] or '' ).gsub('%40', '@')}
#        medium ${( request.match(/[&?]utm_medium[=](?<utm_medium>.*?)(&|$)/) or Hash.new('') )[:utm_medium]}
#        source ${( request.match(/[&?]utm_source[=](?<utm_source>.*?)(&|$)/) or Hash.new('') )[:utm_source]}
#        campaign ${( request.match(/[&?]utm_campaign[=](?<utm_camp>.*?)(&|$)/) or Hash.new('') )[:utm_camp]}
#        u_id ${( request.match(/[&?]user_id[=](?<user_id>.*?)(&|$)/) or Hash.new('') )[:user_id]}
#        job_id ${( ( ( ( request.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[0] or '' ).match(/^(?<job_id>[0-9]+)$/) ) or Hash.new('') )[:job_id]}
#    </record>
#    remove_keys comblog,syslogbase,syslogtimestamp,facility,priority,pid,program,ident,auth,rawrequest,request,httpversion,bytes,referrer,agent,logsource
#</filter>
#
### Filter applications
#<filter apply.*>
#    @type record_transformer
#    enable_ruby
#    <record>
#        campdt ${( ( ( referrer.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[2] or '' ).match(/(?<datepart>.*?)T(.*)/) or Hash.new('') )[:datepart].gsub(/-/, '')}
#        email ${( ( referrer.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[3] or '' ).gsub('%40', '@')}
#        medium ${( referrer.match(/[&?]utm_medium[=](?<utm_medium>.*?)(&|$)/) or Hash.new('') )[:utm_medium]}
#        source ${( referrer.match(/[&?]utm_source[=](?<utm_source>.*?)(&|$)/) or Hash.new('') )[:utm_source]}
#        campaign ${( referrer.match(/[&?]utm_campaign[=](?<utm_camp>.*?)(&|$)/) or Hash.new('') )[:utm_camp]}
#        u_id ${( referrer.match(/[&?]user_id[=](?<user_id>.*?)(&|$)/) or Hash.new('') )[:user_id]}
#        job_id ${( ( ( ( referrer.match(/[&?]etm_content[=](?<etm_content>.*?)(&|$)/) or Hash.new('') )[:etm_content].gsub("%7C", "|").split("|")[0] or '' ).match(/^(?<job_id>[0-9]+)$/) ) or Hash.new('') )[:job_id]}
#    </record>
#    remove_keys comblog,syslogbase,syslogtimestamp,facility,priority,pid,program,ident,auth,rawrequest,request,httpversion,bytes,referrer,agent,logsource
#</filter>
#
#<match clicks>
#    @type elasticsearch
#    host localhost
#    port 9200
#    logstash_format true
#    logstash_prefix logstash-clicks
#    logstash_dateformat %Y.%m
#    type_name click
#    flush_interval 10s
#    time_key timestamp
#</match>
#
#<match apply.*>
#    @type elasticsearch
#    host localhost
#    port 9200
#    logstash_format true
#    logstash_prefix logstash-applies
#    logstash_dateformat %Y.%m
#    type_name apply
#    flush_interval 10s
#    time_key timestamp
#</match>
